vllm:
  enable: true
  replicas: 1
  image: "ccr.ccs.tencentyun.com/tke-ai-playbook/vllm-openai:v0.9.0.1"
  imagePullPolicy: IfNotPresent
  cmd: |
    vllm serve /data/models/Qwen/Qwen3-32B \
      --served-model-name Qwen/Qwen3-32B \
      --block-size 128 \
      --max-model-len 16384 \
      --max-num-batched-tokens 16384 \
      --enable-chunked-prefill \
      -tp 4 \
      --disable-log-requests \
      -O '{"cudagraph_capture_sizes": [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]}'
  resources:
    requests:
      nvidia.com/gpu: 4
    limits:
      nvidia.com/gpu: 4
  service:
    enable: true
    type: ClusterIP
    port: 8000
    targetPort: 8000

modelVolume:
  hostPath:
    path: /data0
