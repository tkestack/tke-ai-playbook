apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen3-32b-vllm
  labels:
    app: qwen3-32b-vllm
spec:
  replicas: 4
  selector:
    matchLabels:
      app: qwen3-32b-vllm
  template:
    metadata:
      labels:
        app: qwen3-32b-vllm
    spec:
      containers:
      - args:
        - |
          vllm serve /data/models/Qwen/Qwen3-32B \
            --served-model-name Qwen/Qwen3-32B \
            --block-size 128 \
            --max-model-len 4096 \
            --max-num-batched-tokens 4096 \
            --no-enable-prefix-caching \
            --enable-chunked-prefill \
            --compilation-config '{"cudagraph_capture_sizes": [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]}' \
            -tp 4 \
            --disable-log-requests
        command:
        - bash
        - -c
        env:
        - name: VLLM_WORKER_MULTIPROC_METHOD
          value: spawn
        image: ccr.ccs.tencentyun.com/tke-ai-playbook/vllm-openai:v0.9.0.1
        imagePullPolicy: Always
        name: vllm
        resources:
          limits:
            nvidia.com/gpu: "4"
          requests:
            nvidia.com/gpu: "4"
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /data/models
          name: models
      dnsPolicy: ClusterFirstWithHostNet
      hostIPC: true
      hostNetwork: true
      restartPolicy: Always
      securityContext:
        runAsUser: 0
      volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 15Gi
        name: dshm
      - configMap:
          name: qwen3-32b-mixed-configs
        name: configs
      - configMap:
          name: qwen3-32b-mixed-graphs
        name: graphs
      - hostPath:
          path: /data0
        name: models
---
apiVersion: v1
kind: Service
metadata:
  name: qwen3-32b-vllm
spec:
  type: ClusterIP
  ports:
  - name: serve
    port: 8000
    protocol: TCP
    targetPort: 8000
  selector:
    app: qwen3-32b-vllm
---
apiVersion: v1
kind: Pod
metadata:
  name: qwen3-32b-vllm-llmbench
spec:
  containers:
  - args:
    - |
      export HOST=qwen3-32b-vllm
      until curl -sf http://${HOST}:8000/health; do sleep 10; done;
      sleep 60;
      export MODEL=Qwen/Qwen3-32B
      export TOKENIZER=/data/Qwen/Qwen3-32B
      bash bench.sh
      ISL=2500 OSL=100 bash benchmark_serving_concurrency.sh
      tail -f /dev/null
    command:
    - bash
    - -c
    image: ccr.ccs.tencentyun.com/tke-ai-playbook/llmbench:nightly
    imagePullPolicy: Always
    name: bench
    resources:
      limits:
        cpu: "10"
        memory: 20Gi
      requests:
        cpu: "10"
        memory: 20Gi
    volumeMounts:
    - mountPath: /data
      name: models
  enableServiceLinks: false
  volumes:
  - hostPath:
      path: /data0
    name: models