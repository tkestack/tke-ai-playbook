llmbench:
  enable: true
  service: mixed
  model: Qwen/Qwen3-32B

deployments:
- name: mixed 
  image: "ccr.ccs.tencentyun.com/tke-ai-playbook/dynamo:v0.3.0-lws-pr1333"
  imagePullPolicy: Always
  debug: false
  replicas: 1
  resources:
    requests:
      nvidia.com/gpu: 8
    limits:
      nvidia.com/gpu: 8
  service:
    enable: true
    port: 8000
  component: "Frontend"
  workspace: /workspace/examples/llm
  graphs: |
    from components.frontend import Frontend
    from components.processor import Processor
    from components.worker import VllmWorker

    Frontend.link(Processor).link(VllmWorker)
  configs: |
    Common:
      model: /data/models/Qwen/Qwen3-32B
      block-size: 128
      max-model-len: 4096
      kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'
      max-num-batched-tokens: 4096

    Frontend:
      served_model_name: Qwen/Qwen3-32B
      endpoint: dynamo.Processor.chat/completions
      port: 8000

    Processor:
      router: round-robin
      common-configs: [model, block-size]

    VllmWorker:
      remote-prefill: true
      conditional-disagg: false
      compilation-config: '{"cudagraph_capture_sizes": [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]}'
      tensor-parallel-size: 4
      ServiceArgs:
        workers: 2
        resources:
          gpu: '4'
      common-configs: [model, block-size, max-model-len, max-num-batched-tokens, kv-transfer-config]

- name: prefillworker
  image: "ccr.ccs.tencentyun.com/tke-ai-playbook/dynamo:v0.3.0-lws-pr1333"
  imagePullPolicy: Always
  debug: false
  replicas: 1
  resources:
    requests:
      nvidia.com/gpu: 8
    limits:
      nvidia.com/gpu: 8
  service:
    enable: false
  component: "PrefillWorker"
  workspace: /workspace/examples/llm
  graphs: |
    from components.prefill_worker import PrefillWorker
  configs: |
    Common:
      model: /data/models/Qwen/Qwen3-32B
      block-size: 128
      max-model-len: 4096
      kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'
      max-num-batched-tokens: 4096

    PrefillWorker:
      enforce-eager: true
      tensor-parallel-size: 1
      ServiceArgs:
        workers: 8
        resources:
          gpu: '1'
      common-configs: [model, block-size, max-model-len, max-num-batched-tokens, kv-transfer-config]

modelVolume:
  hostPath:
    path: /data0

nats:
  enable: true
  config:
    jetstream:
      enabled: true
      fileStore:
        enabled: true
        dir: /data
        pvc:
          enabled: true
          size: 10Gi

etcd:
  enable: true
  replicaCount: 1
  # Explicitly remove authentication
  auth:
    rbac:
      create: false

  readinessProbe:
    enabled: false

  livenessProbe:
    enabled: false

  persistence:
    size: 20Gi
    resources:
    requests:
      cpu: 10
      memory: 20Gi
    limits:
      cpu: 10
      memory: 20Gi
