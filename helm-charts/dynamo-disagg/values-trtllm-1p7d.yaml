llmbench:
  enable: true
  service: mixed
  model: Qwen/Qwen3-32B

deployments:
- name: mixed 
  image: "tkeai.tencentcloudcr.com/tke-ai-playbook/dynamo:nightly-trtllm"
  imagePullPolicy: IfNotPresent
  debug: false
  replicas: 1
  resources:
    requests:
      nvidia.com/gpu: 8
    limits:
      nvidia.com/gpu: 8
  service:
    enable: true
    port: 8000
  component: "Frontend"
  workspace: /workspace/examples/tensorrt_llm
  graphs: |
    from components.frontend import Frontend
    from components.prefill_worker import TensorRTLLMPrefillWorker
    from components.worker import TensorRTLLMWorker

    Frontend.link(TensorRTLLMWorker).link(TensorRTLLMPrefillWorker)
  configs: |
    Frontend:
      served_model_name: /data/models/Qwen/Qwen3-32B
      block-size: 128
      endpoint: dynamo.TensorRTLLMWorker.generate
      port: 8000
      router: round-robin

    TensorRTLLMWorker:
      served_model_name: /data/models/Qwen/Qwen3-32B
      block-size: 128
      engine_args: "configs/llm_api_config.yaml"
      llmapi-disaggregated-config: "configs/single_node_config.yaml"
      router: round-robin
      remote-prefill: true
      min-prefill-workers: 1
      ServiceArgs:
        workers: 3
        resources:
          gpu: 2

    TensorRTLLMPrefillWorker:
      served_model_name: /data/models/Qwen/Qwen3-32B
      block-size: 128
      engine_args: "configs/llm_api_config.yaml"
      llmapi-disaggregated-config: "configs/single_node_config.yaml"
      router: round-robin
      ServiceArgs:
        workers: 1
        resources:
          gpu: 2
  additionalConfigs:
  - name: llm_api_config.yaml
    content: |
      backend: pytorch
      model_name: "/data/models/Qwen/Qwen3-32B"
      model_path: "/data/models/Qwen/Qwen3-32B"
      tensor_parallel_size: 2
      enable_attention_dp: false
      max_num_tokens: 4096
      kv_cache_config:
        free_gpu_memory_fraction: 0.95
      disable_overlap_scheduler: true
      use_cuda_graph: false
      print_iter_log: true
  - name: single_node_config.yaml
    content: |
      backend: pytorch
      context_servers:
        num_instances: 1
        tensor_parallel_size: 2
        max_num_tokens: 4096
        max_batch_size: 4
        disable_overlap_scheduler: true
        use_cuda_graph: false
      generation_servers:
        num_instances: 1
        tensor_parallel_size: 2
        max_num_tokens: 16384
        max_batch_size: 128
        disable_overlap_scheduler: false
        use_cuda_graph: true
        cuda_graph_padding_enabled: true
        cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]

- name: prefillworker
  image: "tkeai.tencentcloudcr.com/tke-ai-playbook/dynamo:nightly-trtllm"
  imagePullPolicy: IfNotPresent
  debug: false
  replicas: 1
  resources:
    requests:
      nvidia.com/gpu: 8
    limits:
      nvidia.com/gpu: 8
  service:
    enable: false
  component: "TensorRTLLMWorker"
  workspace: /workspace/examples/tensorrt_llm
  graphs: |
    from components.worker import TensorRTLLMWorker
  configs: |
    TensorRTLLMWorker:
      served_model_name: /data/models/Qwen/Qwen3-32B
      block-size: 128
      engine_args: "configs/llm_api_config.yaml"
      llmapi-disaggregated-config: "configs/single_node_config.yaml"
      router: round-robin
      remote-prefill: true
      min-prefill-workers: 1
      ServiceArgs:
        workers: 4
        resources:
          gpu: 2
  additionalConfigs:
  - name: llm_api_config.yaml
    content: |
      backend: pytorch
      model_name: "/data/models/Qwen/Qwen3-32B"
      model_path: "/data/models/Qwen/Qwen3-32B"
      tensor_parallel_size: 2
      enable_attention_dp: false
      max_num_tokens: 4096
      kv_cache_config:
        free_gpu_memory_fraction: 0.95
      disable_overlap_scheduler: true
      use_cuda_graph: false
      print_iter_log: true
  - name: single_node_config.yaml
    content: |
      backend: pytorch
      context_servers:
        num_instances: 1
        tensor_parallel_size: 2
        max_num_tokens: 4096
        max_batch_size: 4
        disable_overlap_scheduler: true
        use_cuda_graph: false
      generation_servers:
        num_instances: 1
        tensor_parallel_size: 2
        max_num_tokens: 32768
        max_batch_size: 256
        disable_overlap_scheduler: false
        use_cuda_graph: true
        cuda_graph_padding_enabled: true
        cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]

modelVolume:
  hostPath:
    path: /data0

nats:
  enable: true
  config:
    jetstream:
      enabled: true
      fileStore:
        enabled: true
        dir: /data
        pvc:
          enabled: true
          size: 10Gi

etcd:
  enable: true
  replicaCount: 1
  # Explicitly remove authentication
  auth:
    rbac:
      create: false

  readinessProbe:
    enabled: false

  livenessProbe:
    enabled: false

  persistence:
    size: 20Gi
    resources:
    requests:
      cpu: 10
      memory: 20Gi
    limits:
      cpu: 10
      memory: 20Gi
