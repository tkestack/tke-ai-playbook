apiVersion: v1
kind: Pod
metadata:
  name: llmbench-qwen3-32b-vllm-2500-100
spec:
  containers:
  - args:
    - |
      export HOST=qwen3-32b-vllm
      export MODEL=Qwen/Qwen3-32B
      export TOKENIZER=/data/Qwen/Qwen3-32B
      ISL=2500 OSL=100 bash benchmark_serving_concurrency.sh
      tail -f /dev/null
    command:
    - bash
    - -c
    image: ccr.ccs.tencentyun.com/tke-ai-playbook/llmbench:nightly
    imagePullPolicy: Always
    name: bench
    resources:
      limits:
        cpu: "10"
        memory: 20Gi
      requests:
        cpu: "10"
        memory: 20Gi
    volumeMounts:
    - mountPath: /data
      name: models
  enableServiceLinks: false
  volumes:
  - hostPath:
      path: /data0
    name: models